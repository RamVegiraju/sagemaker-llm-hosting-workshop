# sagemaker-llm-hosting-workshop
Workshop for getting started with Hosting LLMs on SageMaker.

## Content

- [PyTorch Traditional Model Deployment](): In this section we focus on deploying a traditional pre-trained PyTorch Model to SageMaker Real-Time Inference.
- [Single LLM Deployment](): We take a Llama 7B example with the LMI container and how to load test the deployed endpoint while enabling AutoScaling
- [Multi-LLM Deployment](): For use-cases where you have multiple LLMs or models we show how you can use Inference Components with SageMaker to efficiently allocate resources and host multiple models on a singular endpoint.