{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cf78890-3526-45dc-91ad-20efadcc3a0c",
   "metadata": {},
   "source": [
    "# PyTorch Pre-Trained Model Deployment Example\n",
    "\n",
    "In this example we'll take a look at taking a pre-trained SageMaker PyTorch example and deploying it on SageMaker Real-Time Inference. We'll take a sample local PyTorch model train it on artifical data and then deploy that trained model artifact to a SageMaker Endpoint. The idea here is to show the general SageMaker deplyoment flow utilizing the AWS Boto3 Python SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c2dc9-1b73-4146-9c19-94c34cff97bd",
   "metadata": {},
   "source": [
    "## Local Model Training\n",
    "\n",
    "Taking a sample PyTorch model for local training, we will take the serialized model artifacts and deploy them for inference. In this case the model artifacts (model.pth) is what we will generate these varies depending on the model. For instance SKLearn might have a model.joblib whereas for LLMs they'll have a variety of metadata files (.json, tensors, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd68d1-42ef-483c-adfa-05341a0b044c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# artificial data for lin reg\n",
    "torch.manual_seed(42)\n",
    "X = 3 * torch.rand(100, 1)\n",
    "y = 3 * X + 2 + 0.1 * torch.randn(100, 1)\n",
    "\n",
    "# lin reg model\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# train model\n",
    "model = LinearRegressionModel()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5f229f-c97a-4794-bf1b-f118f7f4859a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# serialize model data\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a926cb88-8073-4519-950f-ff6b6eaf37c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "loaded_model = LinearRegressionModel()\n",
    "loaded_model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "# sample inference\n",
    "samp_data = [[2.5]]\n",
    "with torch.no_grad():\n",
    "    prediction = loaded_model(torch.tensor(samp_data))\n",
    "output = prediction.tolist()\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185c154b-88bf-4442-b70d-257fc8cf1d44",
   "metadata": {},
   "source": [
    "## SageMaker Deployment\n",
    "\n",
    "For SageMaker Deployment there are a few key constructs:\n",
    "\n",
    "- SageMaker Model Object: Points towards model data and any inference artifacts.\n",
    "- SageMaker Endpoint Configuration: Defines hardware for the model.\n",
    "- SageMaker Endpoint: The persistent REST Endpoint for invocation, can attach AutoScaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef4d203-5860-4fd9-82b0-fdbb74bfb3b2",
   "metadata": {},
   "source": [
    "### Custom Inference Script\n",
    "\n",
    "In cases you want to control model loading, pre/post processing you can define your own inference scripts to override the default handlers of the model server that the container exposes. In this case these are the four functions for PyTorch that can be overriden:\n",
    "\n",
    "- model_fn: Load model\n",
    "- input_fn: Handle input + preprocessing\n",
    "- output_fn: Handle output and structure it necessarily\n",
    "- predict_fn: Control model inference\n",
    "\n",
    "We attach this inference script to our model data and package it into a model.tar.gz that SageMaker expects. The packaging of this model.tar.gz is dependent on the model server/container you are using, each model server expects a different file structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f335ab01-016d-4b93-a6e6-21ec41288015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile inference.py\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "\n",
    "# load model\n",
    "def model_fn(model_dir): \n",
    "    model = LinearRegressionModel()   \n",
    "    with open(os.path.join(model_dir, \"model.pth\"), \"rb\") as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "    return model\n",
    "\n",
    "# preprocessing input\n",
    "def input_fn(request_body, request_content_type):\n",
    "    assert request_content_type == \"application/json\"\n",
    "    data = json.loads(request_body)[\"inputs\"]\n",
    "    input_data = torch.tensor(data)\n",
    "    return input_data\n",
    "\n",
    "# inference\n",
    "def predict_fn(input_object, model):\n",
    "    with torch.no_grad():\n",
    "        prediction = model(input_object)\n",
    "    output = prediction.tolist()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fef0361-97f8-4fa9-9e30-918023f50a2d",
   "metadata": {},
   "source": [
    "### SageMaker Objects Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fa8ae0-df7c-4c9f-9cfd-81424b660a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import joblib\n",
    "import pickle\n",
    "import tarfile\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "import subprocess\n",
    "\n",
    "#Setup\n",
    "client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "boto_session = boto3.session.Session()\n",
    "s3 = boto_session.resource('s3')\n",
    "region = boto_session.region_name\n",
    "print(region)\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2fab87-4d24-4623-b824-7f238966b656",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Build tar file with model data + inference code\n",
    "bashCommand = \"tar -cvpzf model.tar.gz model.pth inference.py\"\n",
    "process = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\n",
    "output, error = process.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b109086-774b-4feb-bc0e-9650fa87c42b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# retrieve pytorch image\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"pytorch\",\n",
    "    region=region,\n",
    "    version=\"2.1\",\n",
    "    py_version=\"py310\",\n",
    "    image_scope=\"inference\",\n",
    "    instance_type=\"ml.m5.xlarge\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68958563-5021-48c8-8be5-c2519fd957cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Bucket for model artifacts\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "print(default_bucket)\n",
    "\n",
    "#Upload tar.gz to bucket\n",
    "model_artifacts = f\"s3://{default_bucket}/model.tar.gz\"\n",
    "response = s3.meta.client.upload_file('model.tar.gz', default_bucket, 'model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a625ee1-4f46-48a1-93e7-3c6e1d830913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 1: Model Creation\n",
    "model_name = \"pytorch-test\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Model name: \" + model_name)\n",
    "create_model_response = client.create_model(\n",
    "    ModelName=model_name,\n",
    "    Containers=[\n",
    "        {\n",
    "            \"Image\": image_uri,\n",
    "            \"Mode\": \"SingleModel\",\n",
    "            \"ModelDataUrl\": model_artifacts,\n",
    "            \"Environment\": {'SAGEMAKER_SUBMIT_DIRECTORY': model_artifacts,\n",
    "                           'SAGEMAKER_PROGRAM': 'inference.py'} \n",
    "        }\n",
    "    ],\n",
    "    ExecutionRoleArn=role,\n",
    ")\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f344b2-4c71-444f-b734-e12678cc84dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 2: EPC Creation\n",
    "epc_name = \"pt-epc\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName=epc_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"ptvariant\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.c5.large\",\n",
    "            \"InitialInstanceCount\": 1\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f279072-5857-4546-a35a-173f406aa81b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Step 3: EP Creation\n",
    "endpoint_name = \"pt-ep\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=epc_name,\n",
    ")\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d170b96-cda4-412b-bfe0-65ed1515d1ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Monitor creation\n",
    "describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "    time.sleep(15)\n",
    "print(describe_endpoint_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801e848a-8ffe-4af7-a61e-d30bf65a1dcb",
   "metadata": {},
   "source": [
    "## Sample Inference\n",
    "\n",
    "You can use the [SDK runtime client](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker-runtime/client/invoke_endpoint.html) to directly invoke the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f787d0-3e2d-46fd-a486-f8f6d0e66926",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "content_type = \"application/json\"\n",
    "request_body = {\"inputs\": [[2.5]]}\n",
    "data = json.loads(json.dumps(request_body))\n",
    "payload = json.dumps(data)\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=payload)\n",
    "result = json.loads(response['Body'].read().decode())\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
