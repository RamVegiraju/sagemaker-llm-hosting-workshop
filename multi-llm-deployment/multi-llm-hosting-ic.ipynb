{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a77ca71-2e6c-4bee-b7ef-7965aa904e70",
   "metadata": {},
   "source": [
    "# Utilizing SageMaker Inference Components To Host Multiple LLMs\n",
    "\n",
    "In this example we will take a look at using SageMaker Inference Components to host both the Llama 7B model we deployed in the SME Lab and a Flan T-5 Model. With Inference Components you can bring multiple containers onto a singular endpoint. In this case we have two different models with different containers/model servers implemented. You can also optionally bring your own container.\n",
    "\n",
    "The flow for creating Inference Components is a little different from creating a traditional SageMaker Endpoint.\n",
    "\n",
    "![creation-flow](images/ic-arch.png)\n",
    "\n",
    "Think of an IC Component as a combination of two factors:\n",
    "\n",
    "- <b>SageMaker Model Object</b>: Model data + container selection\n",
    "- <b>Hardware Resources</b>: Dedicated Compute you are assigning to that Model (GPUs, Inferentia2, CPU).\n",
    "    - <b>Copy Count</b>: Number of copies of a model, you can set AutoScaling policy at a per model level based off of copy count.\n",
    "    \n",
    "\n",
    "To understand scaling at a per model level please reference this [example](https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/llm-workshop/lab-inference-components-with-scaling/2c_meta-llama2-7b-lmi-autoscaling.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4039d5ba-2ef6-4b62-8893-7a42698278cb",
   "metadata": {},
   "source": [
    "## Setup & Endpoint Creation\n",
    "\n",
    "To get started we create a persistent SageMaker Endpoint and enable managed AutoScaling at the endpoint level. Here AutoScaling is taken care for us at the endpoint level and you can enable AutoScaling policies at a model/container level based off of the number of invocations per copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb593a-d2f3-4d86-abb9-66f23f3ebd36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install sagemaker --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3bb8c-72b8-4305-8f8d-be0684b28e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "\n",
    "#Setup\n",
    "client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "boto_session = boto3.session.Session()\n",
    "s3 = boto_session.resource('s3')\n",
    "region = boto_session.region_name\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "print(f\"Role ARN: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "\n",
    "# client setup\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0333a252-1cf6-45e6-8938-f8c17dd5c63d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoint config name\n",
    "epc_name = \"ic-epc\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(f\"Endpoint Config Name: {epc_name}\")\n",
    "\n",
    "# Container Parameters, increase health check for LLMs: \n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.48xlarge\"\n",
    "model_data_download_timeout_in_seconds = 3600\n",
    "container_startup_health_check_timeout_in_seconds = 3600\n",
    "\n",
    "# Setting up managed AutoScaling at endpoint level\n",
    "initial_instance_count = 1\n",
    "max_instance_count = 4\n",
    "print(f\"Initial instance count: {initial_instance_count}\")\n",
    "print(f\"Max instance count: {max_instance_count}\")\n",
    "\n",
    "# Endpoint Config Creation\n",
    "endpoint_config_response = client.create_endpoint_config(\n",
    "    EndpointConfigName=epc_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": initial_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            # can set to least outstanding or random: https://aws.amazon.com/blogs/machine-learning/minimize-real-time-inference-latency-by-using-amazon-sagemaker-routing-strategies/\n",
    "            \"RoutingConfig\": {\"RoutingStrategy\": \"LEAST_OUTSTANDING_REQUESTS\"},\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Endpoint Configuration Arn: \" + endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c32fbc1-19ad-4eee-9c77-ea9c929bd887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Endpoint Creation\n",
    "endpoint_name = \"ic-ep\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "create_endpoint_response = client.create_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    EndpointConfigName=epc_name,\n",
    ")\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684ffb0f-e2b7-4f77-805b-3143597bedcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Monitor creation\n",
    "describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "while describe_endpoint_response[\"EndpointStatus\"] == \"Creating\":\n",
    "    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    print(describe_endpoint_response[\"EndpointStatus\"])\n",
    "    time.sleep(15)\n",
    "print(describe_endpoint_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490b01cf-ef3b-4b2d-9195-54a45ed40f91",
   "metadata": {},
   "source": [
    "## Inference Components Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af57253-46ac-4bc1-828e-ffe32516471d",
   "metadata": {},
   "source": [
    "### Inference Component 1: Llama 7B via LMI Container\n",
    "\n",
    "Here we'll use our single model Llama 7b optimized example and take the same container to create our Inference Component. We create a SageMaker Model object and the IC inherits the metadata from this object. The new API call we are dealing with is the [create_inference_component API call](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker/client/create_inference_component.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5864a995-9f9e-4b92-a108-710b8fb962e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf code_llama2_7b_fp16\n",
    "!mkdir -p code_llama2_7b_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb6a35-7a16-4c45-af71-8a5a99a2dbad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code_llama2_7b_fp16/serving.properties\n",
    "engine=MPI\n",
    "option.tensor_parallel_degree=4\n",
    "option.rolling_batch=trtllm\n",
    "option.paged_attention = true\n",
    "option.max_rolling_batch_prefill_tokens = 16080\n",
    "option.max_rolling_batch_size=64\n",
    "option.model_loading_timeout = 900\n",
    "option.model_id = s3://sagemaker-example-files-prod-us-east-1/models/llama-2/fp16/7B/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2913045-c9c1-4e49-8a0b-771a95ceea3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "        framework=\"djl-tensorrtllm\",\n",
    "        region=sagemaker_session.boto_session.region_name,\n",
    "        version=\"0.26.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f8d2c1-f8e9-4c4c-899c-7f4b11437de8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm model.tar.gz\n",
    "!tar czvf model.tar.gz code_llama2_7b_fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191d2c40-de21-4755-a535-43cdf8720cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_prefix = \"hf-large-model-djl/meta-llama/Llama-2-7b-fp16/code\"\n",
    "s3_code_artifact = sagemaker_session.upload_data(\"model.tar.gz\", bucket, s3_code_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc4854d-12df-4d54-9706-b0acf80b2980",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Model data is stored: {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7362d08c-2c03-44af-9350-42539dd95755",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "llama_model_name = name_from_base(f\"Llama-2-7b-fp16-mpi\")\n",
    "print(llama_model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=llama_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\"Image\": image_uri, \"ModelDataUrl\": s3_code_artifact},\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c07580-fd18-4d10-bd68-8eafa3b577b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama7b_ic_name = \"llama7b-ic\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "# llama inference component reaction\n",
    "create_llama_ic_response = sm_client.create_inference_component(\n",
    "    InferenceComponentName=llama7b_ic_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": llama_model_name,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            # enables tensor parallel via TGI, reserving 4 GPUs (g5.48xlarge has 8 GPUs)\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 4,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    # can setup autoscaling for copies, each copy will retain the hardware you have allocated\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")\n",
    "\n",
    "print(\"IC Llama Arn: \" + create_llama_ic_response[\"InferenceComponentArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893812a6-e6de-48f8-a36a-cbeb0e73e0ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "describe_ic_llama_response = client.describe_inference_component(\n",
    "    InferenceComponentName=llama7b_ic_name)\n",
    "\n",
    "while describe_ic_llama_response[\"InferenceComponentStatus\"] == \"Creating\":\n",
    "    describe_ic_llama_response = client.describe_inference_component(InferenceComponentName=llama7b_ic_name)\n",
    "    print(describe_ic_llama_response[\"InferenceComponentStatus\"])\n",
    "    time.sleep(100)\n",
    "print(describe_ic_llama_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5608f6ed-8322-4b51-8edf-389f7e4083d2",
   "metadata": {},
   "source": [
    "#### Sample Inference\n",
    "\n",
    "This is the same REST API call, you just specify the necessary inference component name for scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c8afa-a6a5-4379-842c-927e0df0d21b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\"inputs\": \"Who is Roger Federer?\", \n",
    "           \"parameters\": {\"max_new_tokens\":128, \"do_sample\":True}}\n",
    "\n",
    "import json\n",
    "\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "content_type = \"application/json\"\n",
    "\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=llama7b_ic_name, #specify IC name\n",
    "    ContentType=content_type,\n",
    "    Body=json.dumps(payload))\n",
    "result = json.loads(response['Body'].read().decode())['generated_text']\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc27c01e-eee8-4c94-81ed-92443479e9c2",
   "metadata": {},
   "source": [
    "### Inference Component 2: FlanT5 via TGI Container\n",
    "\n",
    "In the case of our second Inference Component we use the HuggingFace Text Generation Inference (TGI) container to pull down the Flan T-5 model directly. To understand which model server/container to use for your LLM hosting and the tradeoffs please refer to the following [article](https://aws.plainenglish.io/four-different-ways-to-host-large-language-models-on-amazon-sagemaker-4d1b027812b5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5711a731-3bde-4671-8461-edcb0282ac29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "import json\n",
    "\n",
    "# utilizing huggingface TGI container\n",
    "image_uri = get_huggingface_llm_image_uri(\"huggingface\",version=\"1.1.0\")\n",
    "print(f\"TGI Image: {image_uri}\")\n",
    "\n",
    "# Flan T5 TGI Model\n",
    "flant5_model = {\"Image\": image_uri, \"Environment\": {\"HF_MODEL_ID\": \"google/flan-t5-xxl\"}}\n",
    "flant5_model_name = \"flant5-model\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(f\"Flan Model Name: {flant5_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fd13ab-a74c-469c-95ad-7378c1b7e5c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create model object for flan t5\n",
    "create_flan_model_response = sm_client.create_model(\n",
    "    ModelName=flant5_model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    Containers=[flant5_model],\n",
    ")\n",
    "print(\"Flan Model Arn: \" + create_flan_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e2200a-ffec-4121-ab78-5a9c7e884709",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flant5_ic_name = \"flant5-ic\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "variant_name = \"AllTraffic\"\n",
    "\n",
    "# flan inference component reaction\n",
    "create_flan_ic_response = sm_client.create_inference_component(\n",
    "    InferenceComponentName=flant5_ic_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": flant5_model_name,\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            # enables tensor parallel via TGI, reserving 2 GPUs (g5.48xlarge has 8 GPUs)\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 2,\n",
    "            \"NumberOfCpuCoresRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    # can setup autoscaling for copies\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")\n",
    "\n",
    "print(\"IC Flan Arn: \" + create_flan_ic_response[\"InferenceComponentArn\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d2c58-fc0a-4536-beaa-e484ebb0b8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "describe_ic_flan_response = client.describe_inference_component(\n",
    "    InferenceComponentName=flant5_ic_name)\n",
    "\n",
    "while describe_ic_flan_response[\"InferenceComponentStatus\"] == \"Creating\":\n",
    "    describe_ic_flan_response = client.describe_inference_component(InferenceComponentName=flant5_ic_name)\n",
    "    print(describe_ic_flan_response[\"InferenceComponentStatus\"])\n",
    "    time.sleep(100)\n",
    "print(describe_ic_flan_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaafa5c5-1dcc-41ce-bc18-9711d8bc11c3",
   "metadata": {},
   "source": [
    "#### Sample Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af6fcf1-5124-4f01-99d2-9469f7d4e17e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "payload = \"What is the capitol of the United States?\"\n",
    "response = runtime_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    InferenceComponentName=flant5_ic_name, #specify IC name\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": payload,\n",
    "            \"parameters\": {\n",
    "                \"early_stopping\": True,\n",
    "                \"length_penalty\": 2.0,\n",
    "                \"max_new_tokens\": 50,\n",
    "                \"temperature\": 1,\n",
    "                \"min_length\": 10,\n",
    "                \"no_repeat_ngram_size\": 3,\n",
    "                },\n",
    "        }\n",
    "    ),\n",
    ")\n",
    "result = json.loads(response[\"Body\"].read().decode())\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
